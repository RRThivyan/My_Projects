{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8065fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e90dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial parameters\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "img_dims = (96,96,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861a497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc09eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image files from the dataset\n",
    "image_files = [f for f in glob.glob(r\"D:\\Computer courses\\AI Projects\\PRAICP-1001-GenderDetc\\Data\\images\" + \"/**/*\", recursive=True) if not os.path.isdir(f)]\n",
    "random.shuffle(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e976a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting images to arrays and labelling the categories\n",
    "for img in image_files:\n",
    "\n",
    "    image = cv2.imread(img)\n",
    "    \n",
    "    image = cv2.resize(image, (img_dims[0],img_dims[1]))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "\n",
    "    label = img.split(os.path.sep)[-2] \n",
    "    if label == \"woman\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "        \n",
    "    labels.append([label]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d1f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "769d001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset for training and validation\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "\n",
    "trainY = to_categorical(trainY, num_classes=2) \n",
    "testY = to_categorical(testY, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c55b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmenting datset \n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba8dff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def build(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\": #Returns a string, either 'channels_first' or 'channels_last'\n",
    "        inputShape = (depth, height, width)\n",
    "        chanDim = 1\n",
    "    \n",
    "    # The axis that should be normalized, after a Conv2D layer with data_format=\"channels_first\", \n",
    "    # set axis=1 in BatchNormalization.\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec4bd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = build(width=img_dims[0], height=img_dims[1], depth=img_dims[2],\n",
    "                            classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9c378f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer = (tf.keras.optimizers.Adam(learning_rate=0.0001)), \n",
    "#               optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9acf879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath = 'gender_detection.model', verbose = 1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss',  \n",
    "                           patience=10,         \n",
    "                           restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01584130",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 - 23s - loss: 0.7562 - accuracy: 0.6917 - val_loss: 0.6878 - val_accuracy: 0.5455 - 23s/epoch - 825ms/step\n",
      "Epoch 2/100\n",
      "28/28 - 22s - loss: 0.6506 - accuracy: 0.7479 - val_loss: 0.6863 - val_accuracy: 0.4978 - 22s/epoch - 798ms/step\n",
      "Epoch 3/100\n",
      "28/28 - 22s - loss: 0.5994 - accuracy: 0.7732 - val_loss: 0.6960 - val_accuracy: 0.5476 - 22s/epoch - 797ms/step\n",
      "Epoch 4/100\n",
      "28/28 - 22s - loss: 0.5265 - accuracy: 0.8052 - val_loss: 0.6622 - val_accuracy: 0.6667 - 22s/epoch - 798ms/step\n",
      "Epoch 5/100\n",
      "28/28 - 22s - loss: 0.4969 - accuracy: 0.8119 - val_loss: 0.6565 - val_accuracy: 0.5887 - 22s/epoch - 795ms/step\n",
      "Epoch 6/100\n",
      "28/28 - 22s - loss: 0.4990 - accuracy: 0.8125 - val_loss: 0.6080 - val_accuracy: 0.7446 - 22s/epoch - 799ms/step\n",
      "Epoch 7/100\n",
      "28/28 - 23s - loss: 0.4678 - accuracy: 0.8355 - val_loss: 0.7569 - val_accuracy: 0.5087 - 23s/epoch - 831ms/step\n",
      "Epoch 8/100\n",
      "28/28 - 22s - loss: 0.4467 - accuracy: 0.8349 - val_loss: 0.8081 - val_accuracy: 0.5130 - 22s/epoch - 792ms/step\n",
      "Epoch 9/100\n",
      "28/28 - 22s - loss: 0.4261 - accuracy: 0.8299 - val_loss: 0.8441 - val_accuracy: 0.5216 - 22s/epoch - 803ms/step\n",
      "Epoch 10/100\n",
      "28/28 - 22s - loss: 0.4152 - accuracy: 0.8488 - val_loss: 0.4919 - val_accuracy: 0.7944 - 22s/epoch - 788ms/step\n",
      "Epoch 11/100\n",
      "28/28 - 22s - loss: 0.3993 - accuracy: 0.8462 - val_loss: 0.6279 - val_accuracy: 0.6472 - 22s/epoch - 782ms/step\n",
      "Epoch 12/100\n",
      "28/28 - 23s - loss: 0.4014 - accuracy: 0.8551 - val_loss: 0.4416 - val_accuracy: 0.8268 - 23s/epoch - 805ms/step\n",
      "Epoch 13/100\n",
      "28/28 - 23s - loss: 0.3621 - accuracy: 0.8770 - val_loss: 0.4590 - val_accuracy: 0.8225 - 23s/epoch - 833ms/step\n",
      "Epoch 14/100\n",
      "28/28 - 23s - loss: 0.3779 - accuracy: 0.8596 - val_loss: 0.4669 - val_accuracy: 0.8074 - 23s/epoch - 816ms/step\n",
      "Epoch 15/100\n",
      "28/28 - 22s - loss: 0.3444 - accuracy: 0.8742 - val_loss: 0.3959 - val_accuracy: 0.8442 - 22s/epoch - 796ms/step\n",
      "Epoch 16/100\n",
      "28/28 - 22s - loss: 0.3464 - accuracy: 0.8697 - val_loss: 0.3472 - val_accuracy: 0.8896 - 22s/epoch - 789ms/step\n",
      "Epoch 17/100\n",
      "28/28 - 22s - loss: 0.3323 - accuracy: 0.8754 - val_loss: 0.3131 - val_accuracy: 0.9091 - 22s/epoch - 787ms/step\n",
      "Epoch 18/100\n",
      "28/28 - 22s - loss: 0.3168 - accuracy: 0.8748 - val_loss: 0.2953 - val_accuracy: 0.9004 - 22s/epoch - 784ms/step\n",
      "Epoch 19/100\n",
      "28/28 - 22s - loss: 0.3381 - accuracy: 0.8709 - val_loss: 0.3283 - val_accuracy: 0.8615 - 22s/epoch - 786ms/step\n",
      "Epoch 20/100\n",
      "28/28 - 22s - loss: 0.3360 - accuracy: 0.8759 - val_loss: 0.2439 - val_accuracy: 0.9026 - 22s/epoch - 799ms/step\n",
      "Epoch 21/100\n",
      "28/28 - 22s - loss: 0.2949 - accuracy: 0.8821 - val_loss: 0.3938 - val_accuracy: 0.8506 - 22s/epoch - 790ms/step\n",
      "Epoch 22/100\n",
      "28/28 - 22s - loss: 0.3023 - accuracy: 0.8961 - val_loss: 0.2311 - val_accuracy: 0.9264 - 22s/epoch - 797ms/step\n",
      "Epoch 23/100\n",
      "28/28 - 22s - loss: 0.2693 - accuracy: 0.8984 - val_loss: 0.2233 - val_accuracy: 0.9307 - 22s/epoch - 789ms/step\n",
      "Epoch 24/100\n",
      "28/28 - 22s - loss: 0.2923 - accuracy: 0.8922 - val_loss: 0.2790 - val_accuracy: 0.8918 - 22s/epoch - 787ms/step\n",
      "Epoch 25/100\n",
      "28/28 - 22s - loss: 0.3029 - accuracy: 0.8911 - val_loss: 0.1934 - val_accuracy: 0.9372 - 22s/epoch - 788ms/step\n",
      "Epoch 26/100\n",
      "28/28 - 23s - loss: 0.2904 - accuracy: 0.8950 - val_loss: 0.1998 - val_accuracy: 0.9134 - 23s/epoch - 812ms/step\n",
      "Epoch 27/100\n",
      "28/28 - 23s - loss: 0.2904 - accuracy: 0.8956 - val_loss: 0.1617 - val_accuracy: 0.9502 - 23s/epoch - 804ms/step\n",
      "Epoch 28/100\n",
      "28/28 - 22s - loss: 0.2581 - accuracy: 0.9034 - val_loss: 0.2697 - val_accuracy: 0.8961 - 22s/epoch - 796ms/step\n",
      "Epoch 29/100\n",
      "28/28 - 22s - loss: 0.2798 - accuracy: 0.8984 - val_loss: 0.2064 - val_accuracy: 0.9286 - 22s/epoch - 796ms/step\n",
      "Epoch 30/100\n",
      "28/28 - 22s - loss: 0.2561 - accuracy: 0.9062 - val_loss: 0.1916 - val_accuracy: 0.9307 - 22s/epoch - 795ms/step\n",
      "Epoch 31/100\n",
      "28/28 - 22s - loss: 0.2445 - accuracy: 0.9090 - val_loss: 0.1875 - val_accuracy: 0.9372 - 22s/epoch - 794ms/step\n",
      "Epoch 32/100\n",
      "28/28 - 22s - loss: 0.2373 - accuracy: 0.9107 - val_loss: 0.1767 - val_accuracy: 0.9437 - 22s/epoch - 790ms/step\n",
      "Epoch 33/100\n",
      "28/28 - 23s - loss: 0.2479 - accuracy: 0.9079 - val_loss: 0.2540 - val_accuracy: 0.9048 - 23s/epoch - 807ms/step\n",
      "Epoch 34/100\n",
      "28/28 - 22s - loss: 0.2272 - accuracy: 0.9163 - val_loss: 0.3210 - val_accuracy: 0.8939 - 22s/epoch - 799ms/step\n",
      "Epoch 35/100\n",
      "28/28 - 23s - loss: 0.2325 - accuracy: 0.9163 - val_loss: 0.2866 - val_accuracy: 0.9004 - 23s/epoch - 808ms/step\n",
      "Epoch 36/100\n",
      "28/28 - 22s - loss: 0.2114 - accuracy: 0.9253 - val_loss: 0.5027 - val_accuracy: 0.8528 - 22s/epoch - 795ms/step\n",
      "Epoch 37/100\n",
      "28/28 - 22s - loss: 0.2320 - accuracy: 0.9175 - val_loss: 0.3699 - val_accuracy: 0.8788 - 22s/epoch - 785ms/step\n",
      "Epoch 38/100\n",
      "28/28 - 22s - loss: 0.2394 - accuracy: 0.9113 - val_loss: 0.1909 - val_accuracy: 0.9372 - 22s/epoch - 788ms/step\n",
      "Epoch 39/100\n",
      "28/28 - 22s - loss: 0.2169 - accuracy: 0.9124 - val_loss: 0.1687 - val_accuracy: 0.9437 - 22s/epoch - 784ms/step\n",
      "Epoch 40/100\n",
      "28/28 - 22s - loss: 0.2089 - accuracy: 0.9220 - val_loss: 0.1500 - val_accuracy: 0.9610 - 22s/epoch - 791ms/step\n",
      "Epoch 41/100\n",
      "28/28 - 22s - loss: 0.1951 - accuracy: 0.9315 - val_loss: 0.2017 - val_accuracy: 0.9264 - 22s/epoch - 790ms/step\n",
      "Epoch 42/100\n",
      "28/28 - 22s - loss: 0.2014 - accuracy: 0.9270 - val_loss: 0.2073 - val_accuracy: 0.9307 - 22s/epoch - 783ms/step\n",
      "Epoch 43/100\n",
      "28/28 - 22s - loss: 0.2052 - accuracy: 0.9304 - val_loss: 0.1563 - val_accuracy: 0.9524 - 22s/epoch - 793ms/step\n",
      "Epoch 44/100\n",
      "28/28 - 22s - loss: 0.1882 - accuracy: 0.9304 - val_loss: 0.1844 - val_accuracy: 0.9394 - 22s/epoch - 799ms/step\n",
      "Epoch 45/100\n",
      "28/28 - 22s - loss: 0.2124 - accuracy: 0.9242 - val_loss: 0.1350 - val_accuracy: 0.9654 - 22s/epoch - 792ms/step\n",
      "Epoch 46/100\n",
      "28/28 - 22s - loss: 0.1931 - accuracy: 0.9309 - val_loss: 0.1639 - val_accuracy: 0.9481 - 22s/epoch - 788ms/step\n",
      "Epoch 47/100\n",
      "28/28 - 22s - loss: 0.1871 - accuracy: 0.9259 - val_loss: 0.4456 - val_accuracy: 0.8680 - 22s/epoch - 797ms/step\n",
      "Epoch 48/100\n",
      "28/28 - 22s - loss: 0.1907 - accuracy: 0.9293 - val_loss: 0.4557 - val_accuracy: 0.8571 - 22s/epoch - 788ms/step\n",
      "Epoch 49/100\n",
      "28/28 - 22s - loss: 0.1888 - accuracy: 0.9241 - val_loss: 0.3625 - val_accuracy: 0.8853 - 22s/epoch - 794ms/step\n",
      "Epoch 50/100\n",
      "28/28 - 22s - loss: 0.1712 - accuracy: 0.9399 - val_loss: 0.2194 - val_accuracy: 0.9199 - 22s/epoch - 788ms/step\n",
      "Epoch 51/100\n",
      "28/28 - 22s - loss: 0.1859 - accuracy: 0.9298 - val_loss: 0.2513 - val_accuracy: 0.9069 - 22s/epoch - 786ms/step\n",
      "Epoch 52/100\n",
      "28/28 - 22s - loss: 0.1656 - accuracy: 0.9410 - val_loss: 0.2977 - val_accuracy: 0.8961 - 22s/epoch - 791ms/step\n",
      "Epoch 53/100\n",
      "28/28 - 22s - loss: 0.1820 - accuracy: 0.9349 - val_loss: 0.3005 - val_accuracy: 0.8939 - 22s/epoch - 788ms/step\n",
      "Epoch 54/100\n",
      "28/28 - 22s - loss: 0.1755 - accuracy: 0.9371 - val_loss: 0.3106 - val_accuracy: 0.8983 - 22s/epoch - 785ms/step\n",
      "Epoch 55/100\n",
      "28/28 - 22s - loss: 0.1710 - accuracy: 0.9382 - val_loss: 0.3719 - val_accuracy: 0.8939 - 22s/epoch - 797ms/step\n",
      "Epoch 56/100\n",
      "28/28 - 22s - loss: 0.1823 - accuracy: 0.9315 - val_loss: 0.2291 - val_accuracy: 0.9156 - 22s/epoch - 787ms/step\n",
      "Epoch 57/100\n",
      "28/28 - 22s - loss: 0.1624 - accuracy: 0.9433 - val_loss: 0.2686 - val_accuracy: 0.9048 - 22s/epoch - 788ms/step\n",
      "Epoch 58/100\n",
      "28/28 - 22s - loss: 0.1726 - accuracy: 0.9414 - val_loss: 0.2912 - val_accuracy: 0.8983 - 22s/epoch - 786ms/step\n",
      "Epoch 59/100\n",
      "28/28 - 22s - loss: 0.1575 - accuracy: 0.9472 - val_loss: 0.3758 - val_accuracy: 0.8896 - 22s/epoch - 784ms/step\n",
      "Epoch 60/100\n",
      "28/28 - 22s - loss: 0.1483 - accuracy: 0.9506 - val_loss: 0.3090 - val_accuracy: 0.9004 - 22s/epoch - 793ms/step\n",
      "Epoch 61/100\n",
      "28/28 - 22s - loss: 0.1401 - accuracy: 0.9489 - val_loss: 0.2029 - val_accuracy: 0.9372 - 22s/epoch - 797ms/step\n",
      "Epoch 62/100\n",
      "28/28 - 22s - loss: 0.1565 - accuracy: 0.9405 - val_loss: 0.1016 - val_accuracy: 0.9632 - 22s/epoch - 796ms/step\n",
      "Epoch 63/100\n",
      "28/28 - 23s - loss: 0.1671 - accuracy: 0.9416 - val_loss: 0.1110 - val_accuracy: 0.9654 - 23s/epoch - 806ms/step\n",
      "Epoch 64/100\n",
      "28/28 - 22s - loss: 0.1501 - accuracy: 0.9506 - val_loss: 0.1012 - val_accuracy: 0.9697 - 22s/epoch - 790ms/step\n",
      "Epoch 65/100\n",
      "28/28 - 22s - loss: 0.1385 - accuracy: 0.9455 - val_loss: 0.1547 - val_accuracy: 0.9502 - 22s/epoch - 786ms/step\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 - 22s - loss: 0.1567 - accuracy: 0.9444 - val_loss: 0.3147 - val_accuracy: 0.9091 - 22s/epoch - 785ms/step\n",
      "Epoch 67/100\n",
      "28/28 - 22s - loss: 0.1668 - accuracy: 0.9382 - val_loss: 0.4034 - val_accuracy: 0.8788 - 22s/epoch - 786ms/step\n",
      "Epoch 68/100\n",
      "28/28 - 22s - loss: 0.1408 - accuracy: 0.9422 - val_loss: 0.2789 - val_accuracy: 0.9113 - 22s/epoch - 789ms/step\n",
      "Epoch 69/100\n",
      "28/28 - 22s - loss: 0.1381 - accuracy: 0.9478 - val_loss: 0.1635 - val_accuracy: 0.9416 - 22s/epoch - 786ms/step\n",
      "Epoch 70/100\n",
      "28/28 - 22s - loss: 0.1489 - accuracy: 0.9500 - val_loss: 0.2270 - val_accuracy: 0.9264 - 22s/epoch - 784ms/step\n",
      "Epoch 71/100\n",
      "28/28 - 22s - loss: 0.1562 - accuracy: 0.9467 - val_loss: 0.1099 - val_accuracy: 0.9719 - 22s/epoch - 784ms/step\n",
      "Epoch 72/100\n",
      "28/28 - 22s - loss: 0.1252 - accuracy: 0.9596 - val_loss: 0.2507 - val_accuracy: 0.9242 - 22s/epoch - 784ms/step\n",
      "Epoch 73/100\n",
      "28/28 - 22s - loss: 0.1510 - accuracy: 0.9478 - val_loss: 0.1583 - val_accuracy: 0.9459 - 22s/epoch - 797ms/step\n",
      "Epoch 74/100\n",
      "28/28 - 23s - loss: 0.1079 - accuracy: 0.9641 - val_loss: 0.1888 - val_accuracy: 0.9351 - 23s/epoch - 808ms/step\n",
      "Epoch 75/100\n",
      "28/28 - 23s - loss: 0.1315 - accuracy: 0.9467 - val_loss: 0.3187 - val_accuracy: 0.9113 - 23s/epoch - 808ms/step\n",
      "Epoch 76/100\n",
      "28/28 - 23s - loss: 0.1207 - accuracy: 0.9590 - val_loss: 0.1718 - val_accuracy: 0.9416 - 23s/epoch - 818ms/step\n",
      "Epoch 77/100\n",
      "28/28 - 23s - loss: 0.1343 - accuracy: 0.9461 - val_loss: 0.2955 - val_accuracy: 0.9091 - 23s/epoch - 824ms/step\n",
      "Epoch 78/100\n",
      "28/28 - 23s - loss: 0.1296 - accuracy: 0.9556 - val_loss: 0.3175 - val_accuracy: 0.9113 - 23s/epoch - 822ms/step\n",
      "Epoch 79/100\n",
      "28/28 - 23s - loss: 0.1117 - accuracy: 0.9618 - val_loss: 0.1097 - val_accuracy: 0.9697 - 23s/epoch - 804ms/step\n",
      "Epoch 80/100\n",
      "28/28 - 23s - loss: 0.1150 - accuracy: 0.9607 - val_loss: 0.1926 - val_accuracy: 0.9307 - 23s/epoch - 806ms/step\n",
      "Epoch 81/100\n",
      "28/28 - 24s - loss: 0.1113 - accuracy: 0.9613 - val_loss: 0.1153 - val_accuracy: 0.9632 - 24s/epoch - 844ms/step\n",
      "Epoch 82/100\n",
      "28/28 - 23s - loss: 0.1064 - accuracy: 0.9629 - val_loss: 0.0929 - val_accuracy: 0.9697 - 23s/epoch - 816ms/step\n",
      "Epoch 83/100\n",
      "28/28 - 22s - loss: 0.1177 - accuracy: 0.9585 - val_loss: 0.2201 - val_accuracy: 0.9242 - 22s/epoch - 789ms/step\n",
      "Epoch 84/100\n",
      "28/28 - 22s - loss: 0.1132 - accuracy: 0.9579 - val_loss: 0.2382 - val_accuracy: 0.9242 - 22s/epoch - 790ms/step\n",
      "Epoch 85/100\n",
      "28/28 - 22s - loss: 0.1049 - accuracy: 0.9652 - val_loss: 0.4113 - val_accuracy: 0.8874 - 22s/epoch - 787ms/step\n",
      "Epoch 86/100\n",
      "28/28 - 22s - loss: 0.1045 - accuracy: 0.9635 - val_loss: 0.3716 - val_accuracy: 0.8918 - 22s/epoch - 790ms/step\n",
      "Epoch 87/100\n",
      "28/28 - 22s - loss: 0.1202 - accuracy: 0.9512 - val_loss: 0.2041 - val_accuracy: 0.9372 - 22s/epoch - 793ms/step\n",
      "Epoch 88/100\n",
      "28/28 - 22s - loss: 0.1064 - accuracy: 0.9629 - val_loss: 0.2200 - val_accuracy: 0.9307 - 22s/epoch - 791ms/step\n",
      "Epoch 89/100\n",
      "28/28 - 22s - loss: 0.1156 - accuracy: 0.9579 - val_loss: 0.0957 - val_accuracy: 0.9719 - 22s/epoch - 791ms/step\n",
      "Epoch 90/100\n",
      "28/28 - 22s - loss: 0.1155 - accuracy: 0.9590 - val_loss: 0.0915 - val_accuracy: 0.9762 - 22s/epoch - 789ms/step\n",
      "Epoch 91/100\n",
      "28/28 - 22s - loss: 0.0927 - accuracy: 0.9669 - val_loss: 0.0850 - val_accuracy: 0.9784 - 22s/epoch - 791ms/step\n",
      "Epoch 92/100\n",
      "28/28 - 22s - loss: 0.0987 - accuracy: 0.9624 - val_loss: 0.1229 - val_accuracy: 0.9654 - 22s/epoch - 791ms/step\n",
      "Epoch 93/100\n",
      "28/28 - 22s - loss: 0.0984 - accuracy: 0.9663 - val_loss: 0.1017 - val_accuracy: 0.9654 - 22s/epoch - 789ms/step\n",
      "Epoch 94/100\n",
      "28/28 - 22s - loss: 0.0958 - accuracy: 0.9663 - val_loss: 0.0988 - val_accuracy: 0.9697 - 22s/epoch - 789ms/step\n",
      "Epoch 95/100\n",
      "28/28 - 22s - loss: 0.1119 - accuracy: 0.9573 - val_loss: 0.1092 - val_accuracy: 0.9654 - 22s/epoch - 794ms/step\n",
      "Epoch 96/100\n",
      "28/28 - 22s - loss: 0.0907 - accuracy: 0.9663 - val_loss: 0.1570 - val_accuracy: 0.9481 - 22s/epoch - 797ms/step\n",
      "Epoch 97/100\n",
      "28/28 - 22s - loss: 0.0985 - accuracy: 0.9657 - val_loss: 0.0951 - val_accuracy: 0.9697 - 22s/epoch - 782ms/step\n",
      "Epoch 98/100\n",
      "28/28 - 22s - loss: 0.0889 - accuracy: 0.9686 - val_loss: 0.1153 - val_accuracy: 0.9697 - 22s/epoch - 781ms/step\n",
      "Epoch 99/100\n",
      "28/28 - 22s - loss: 0.0999 - accuracy: 0.9629 - val_loss: 0.0999 - val_accuracy: 0.9610 - 22s/epoch - 782ms/step\n",
      "Epoch 100/100\n",
      "28/28 - 22s - loss: 0.0884 - accuracy: 0.9674 - val_loss: 0.0924 - val_accuracy: 0.9719 - 22s/epoch - 789ms/step\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=batch_size),\n",
    "                        validation_data=(testX,testY),\n",
    "                        steps_per_epoch=len(trainX) // batch_size,\n",
    "                        epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31cd463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "model.save('gender_detection.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa69fee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02435472048819065, 0.9918699264526367]\n",
      "2.435472048819065\n",
      "99.18699264526367\n"
     ]
    }
   ],
   "source": [
    "tscore = model.evaluate(trainX, trainY, verbose=0)\n",
    "taccuracy = 100 * tscore[1]\n",
    "tloss = 100 * tscore[0]\n",
    "print(tscore)\n",
    "print(tloss)\n",
    "print(taccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30e58950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09241681545972824, 0.9718614816665649]\n",
      "9.241681545972824\n",
      "97.1861481666565\n"
     ]
    }
   ],
   "source": [
    "vscore = model.evaluate(testX,testY, verbose=0)\n",
    "vaccuracy = 100 * vscore[1]\n",
    "vloss = 100 * vscore[0]\n",
    "print(vscore)\n",
    "print(vloss)\n",
    "print(vaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30202fec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 316ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import cvlib as cv\n",
    "                    \n",
    "# load model\n",
    "model = load_model('gender_detection.model')\n",
    "\n",
    "# open webcam\n",
    "webcam = cv2.VideoCapture(0)\n",
    "    \n",
    "classes = ['man','woman']\n",
    "\n",
    "# loop through frames\n",
    "while webcam.isOpened():\n",
    "\n",
    "    # read frame from webcam \n",
    "    status, frame = webcam.read()\n",
    "\n",
    "    # apply face detection\n",
    "    face, confidence = cv.detect_face(frame)\n",
    "\n",
    "\n",
    "    # loop through detected faces\n",
    "    for idx, f in enumerate(face):\n",
    "\n",
    "        # get corner points of face rectangle        \n",
    "        (startX, startY) = f[0], f[1]\n",
    "        (endX, endY) = f[2], f[3]\n",
    "\n",
    "        # draw rectangle over face\n",
    "        cv2.rectangle(frame, (startX,startY), (endX,endY), (0,255,0), 2)\n",
    "\n",
    "        # crop the detected face region\n",
    "        face_crop = np.copy(frame[startY:endY,startX:endX])\n",
    "\n",
    "        if (face_crop.shape[0]) < 10 or (face_crop.shape[1]) < 10:\n",
    "            continue\n",
    "\n",
    "        # preprocessing for gender detection model\n",
    "        face_crop = cv2.resize(face_crop, (96,96))\n",
    "        face_crop = face_crop.astype(\"float\") / 255.0\n",
    "        face_crop = img_to_array(face_crop)\n",
    "        face_crop = np.expand_dims(face_crop, axis=0)\n",
    "\n",
    "        # apply gender detection on face\n",
    "        conf = model.predict(face_crop)[0] # model.predict return a 2D matrix, ex: [[9.9993384e-01 7.4850512e-05]]\n",
    "\n",
    "        # get label with max accuracy\n",
    "        idx = np.argmax(conf)\n",
    "        label = classes[idx]\n",
    "\n",
    "        label = \"{}: {:.2f}%\".format(label, conf[idx] * 100)\n",
    "\n",
    "        Y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\n",
    "        # write label and confidence above face rectangle\n",
    "        cv2.putText(frame, label, (startX, Y),  cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # display output\n",
    "    cv2.imshow(\"gender detection\", frame)\n",
    "\n",
    "    # press \"Q\" to stop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release resources\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78092295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
